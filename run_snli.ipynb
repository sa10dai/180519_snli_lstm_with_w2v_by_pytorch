{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run_snli.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[{"file_id":"1S_5ufpA6wNKsqec35GKddQ6Pj7NvWc6e","timestamp":1529721588991}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"IEZlAF0nYo4a","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":2519},"outputId":"b716540e-85c7-4373-a4cf-41606ad99a4c","executionInfo":{"status":"ok","timestamp":1530620487887,"user_tz":-540,"elapsed":29204,"user":{"displayName":"佐藤大輔","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102854220323104846215"}}},"cell_type":"code","source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Preconfiguring packages ...\n","Selecting previously unselected package cron.\n","(Reading database ... 18396 files and directories currently installed.)\n","Preparing to unpack .../00-cron_3.0pl1-128ubuntu5_amd64.deb ...\n","Unpacking cron (3.0pl1-128ubuntu5) ...\n","Selecting previously unselected package libapparmor1:amd64.\n","Preparing to unpack .../01-libapparmor1_2.11.0-2ubuntu17.1_amd64.deb ...\n","Unpacking libapparmor1:amd64 (2.11.0-2ubuntu17.1) ...\n","Selecting previously unselected package libdbus-1-3:amd64.\n","Preparing to unpack .../02-libdbus-1-3_1.10.22-1ubuntu1_amd64.deb ...\n","Unpacking libdbus-1-3:amd64 (1.10.22-1ubuntu1) ...\n","Selecting previously unselected package dbus.\n","Preparing to unpack .../03-dbus_1.10.22-1ubuntu1_amd64.deb ...\n","Unpacking dbus (1.10.22-1ubuntu1) ...\n","Preparing to unpack .../04-gnupg_2.1.15-1ubuntu8.1_amd64.deb ...\n","Unpacking gnupg (2.1.15-1ubuntu8.1) over (2.1.15-1ubuntu8) ...\n","Preparing to unpack .../05-gnupg-agent_2.1.15-1ubuntu8.1_amd64.deb ...\n","Unpacking gnupg-agent (2.1.15-1ubuntu8.1) over (2.1.15-1ubuntu8) ...\n","Selecting previously unselected package dirmngr.\n","Preparing to unpack .../06-dirmngr_2.1.15-1ubuntu8.1_amd64.deb ...\n","Unpacking dirmngr (2.1.15-1ubuntu8.1) ...\n","Selecting previously unselected package distro-info-data.\n","Preparing to unpack .../07-distro-info-data_0.36ubuntu0.2_all.deb ...\n","Unpacking distro-info-data (0.36ubuntu0.2) ...\n","Selecting previously unselected package libkmod2:amd64.\n","Preparing to unpack .../08-libkmod2_24-1ubuntu2_amd64.deb ...\n","Unpacking libkmod2:amd64 (24-1ubuntu2) ...\n","Selecting previously unselected package kmod.\n","Preparing to unpack .../09-kmod_24-1ubuntu2_amd64.deb ...\n","Unpacking kmod (24-1ubuntu2) ...\n","Selecting previously unselected package lsb-release.\n","Preparing to unpack .../10-lsb-release_9.20160110ubuntu5_all.deb ...\n","Unpacking lsb-release (9.20160110ubuntu5) ...\n","Selecting previously unselected package libgirepository-1.0-1:amd64.\n","Preparing to unpack .../11-libgirepository-1.0-1_1.54.1-1_amd64.deb ...\n","Unpacking libgirepository-1.0-1:amd64 (1.54.1-1) ...\n","Selecting previously unselected package gir1.2-glib-2.0:amd64.\n","Preparing to unpack .../12-gir1.2-glib-2.0_1.54.1-1_amd64.deb ...\n","Unpacking gir1.2-glib-2.0:amd64 (1.54.1-1) ...\n","Selecting previously unselected package iso-codes.\n","Preparing to unpack .../13-iso-codes_3.75-1_all.deb ...\n","Unpacking iso-codes (3.75-1) ...\n","Selecting previously unselected package libdbus-glib-1-2:amd64.\n","Preparing to unpack .../14-libdbus-glib-1-2_0.108-2_amd64.deb ...\n","Unpacking libdbus-glib-1-2:amd64 (0.108-2) ...\n","Selecting previously unselected package python-apt-common.\n","Preparing to unpack .../15-python-apt-common_1.4.0~beta3build2_all.deb ...\n","Unpacking python-apt-common (1.4.0~beta3build2) ...\n","Selecting previously unselected package python3-apt.\n","Preparing to unpack .../16-python3-apt_1.4.0~beta3build2_amd64.deb ...\n","Unpacking python3-apt (1.4.0~beta3build2) ...\n","Selecting previously unselected package python3-dbus.\n","Preparing to unpack .../17-python3-dbus_1.2.4-1build3_amd64.deb ...\n","Unpacking python3-dbus (1.2.4-1build3) ...\n","Selecting previously unselected package python3-gi.\n","Preparing to unpack .../18-python3-gi_3.24.1-2build1_amd64.deb ...\n","Unpacking python3-gi (3.24.1-2build1) ...\n","Selecting previously unselected package module-init-tools.\n","Preparing to unpack .../19-module-init-tools_24-1ubuntu2_all.deb ...\n","Unpacking module-init-tools (24-1ubuntu2) ...\n","Selecting previously unselected package python-apt.\n","Preparing to unpack .../20-python-apt_1.4.0~beta3build2_amd64.deb ...\n","Unpacking python-apt (1.4.0~beta3build2) ...\n","Selecting previously unselected package python-pycurl.\n","Preparing to unpack .../21-python-pycurl_7.43.0-2build2_amd64.deb ...\n","Unpacking python-pycurl (7.43.0-2build2) ...\n","Selecting previously unselected package python-software-properties.\n","Preparing to unpack .../22-python-software-properties_0.96.24.17_all.deb ...\n","Unpacking python-software-properties (0.96.24.17) ...\n","Selecting previously unselected package python3-software-properties.\n","Preparing to unpack .../23-python3-software-properties_0.96.24.17_all.deb ...\n","Unpacking python3-software-properties (0.96.24.17) ...\n","Selecting previously unselected package software-properties-common.\n","Preparing to unpack .../24-software-properties-common_0.96.24.17_all.deb ...\n","Unpacking software-properties-common (0.96.24.17) ...\n","Selecting previously unselected package unattended-upgrades.\n","Preparing to unpack .../25-unattended-upgrades_0.98ubuntu1.1_all.deb ...\n","Unpacking unattended-upgrades (0.98ubuntu1.1) ...\n","Setting up python-apt-common (1.4.0~beta3build2) ...\n","Setting up python3-apt (1.4.0~beta3build2) ...\n","Setting up iso-codes (3.75-1) ...\n","Setting up distro-info-data (0.36ubuntu0.2) ...\n","Setting up python-pycurl (7.43.0-2build2) ...\n","Setting up lsb-release (9.20160110ubuntu5) ...\n","Setting up libgirepository-1.0-1:amd64 (1.54.1-1) ...\n","Setting up libkmod2:amd64 (24-1ubuntu2) ...\n","Setting up gir1.2-glib-2.0:amd64 (1.54.1-1) ...\n","Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n"],"name":"stdout"},{"output_type":"stream","text":["Setting up libapparmor1:amd64 (2.11.0-2ubuntu17.1) ...\r\n","Setting up unattended-upgrades (0.98ubuntu1.1) ...\n","\n","Creating config file /etc/apt/apt.conf.d/20auto-upgrades with new version\n","\n","Creating config file /etc/apt/apt.conf.d/50unattended-upgrades with new version\n","invoke-rc.d: could not determine current runlevel\n","invoke-rc.d: policy-rc.d denied execution of start.\n","Setting up gnupg-agent (2.1.15-1ubuntu8.1) ...\n","Setting up dirmngr (2.1.15-1ubuntu8.1) ...\n","Setting up cron (3.0pl1-128ubuntu5) ...\n","Adding group `crontab' (GID 102) ...\n","Done.\n","update-rc.d: warning: start and stop actions are no longer supported; falling back to defaults\n","update-rc.d: warning: stop runlevel arguments (1) do not match cron Default-Stop values (none)\n","invoke-rc.d: could not determine current runlevel\n","invoke-rc.d: policy-rc.d denied execution of start.\n","Setting up libdbus-1-3:amd64 (1.10.22-1ubuntu1) ...\n","Setting up kmod (24-1ubuntu2) ...\n","Setting up libdbus-glib-1-2:amd64 (0.108-2) ...\n","Setting up gnupg (2.1.15-1ubuntu8.1) ...\n","Setting up python3-gi (3.24.1-2build1) ...\n","Setting up module-init-tools (24-1ubuntu2) ...\n","Setting up python3-software-properties (0.96.24.17) ...\n","Setting up dbus (1.10.22-1ubuntu1) ...\n","Setting up python-apt (1.4.0~beta3build2) ...\n","Setting up python3-dbus (1.2.4-1build3) ...\n","Setting up python-software-properties (0.96.24.17) ...\n","Setting up software-properties-common (0.96.24.17) ...\n","Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n","Processing triggers for dbus (1.10.22-1ubuntu1) ...\n","gpg: keybox '/tmp/tmp3uifz2ji/pubring.gpg' created\n","gpg: /tmp/tmp3uifz2ji/trustdb.gpg: trustdb created\n","gpg: key AD5F235DF639B041: public key \"Launchpad PPA for Alessandro Strada\" imported\n","gpg: Total number processed: 1\n","gpg:               imported: 1\n","Warning: apt-key output should not be parsed (stdout is not a terminal)\n","Selecting previously unselected package libfuse2:amd64.\n","(Reading database ... 19804 files and directories currently installed.)\n","Preparing to unpack .../libfuse2_2.9.7-1ubuntu1_amd64.deb ...\n","Unpacking libfuse2:amd64 (2.9.7-1ubuntu1) ...\n","Selecting previously unselected package fuse.\n","Preparing to unpack .../fuse_2.9.7-1ubuntu1_amd64.deb ...\n","Unpacking fuse (2.9.7-1ubuntu1) ...\n","Selecting previously unselected package google-drive-ocamlfuse.\n","Preparing to unpack .../google-drive-ocamlfuse_0.6.21-0ubuntu2_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.6.21-0ubuntu2) ...\n","Setting up libfuse2:amd64 (2.9.7-1ubuntu1) ...\n","Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n","Setting up fuse (2.9.7-1ubuntu1) ...\n","Setting up google-drive-ocamlfuse (0.6.21-0ubuntu2) ...\n"],"name":"stdout"}]},{"metadata":{"id":"KvfcpEVsY_bL","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":109},"outputId":"6d423339-3a01-44db-949b-8456f96a89b9","executionInfo":{"status":"ok","timestamp":1530620603243,"user_tz":-540,"elapsed":13382,"user":{"displayName":"佐藤大輔","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102854220323104846215"}}},"cell_type":"code","source":["from google.colab import auth\n","auth.authenticate_user()\n","\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","\n","import getpass\n","\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"metadata":{"id":"bNC2DLr_ZYiK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9wfuz2bVkszx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import os\n","path = \"drive/practice/colaboratory/180519_snli_lstm_with_w2v_by_pytorch\"\n","os.chdir(path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gzPWDtxHPJjW","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":35},"outputId":"09cc931a-a711-4502-aed8-c44398608a55","executionInfo":{"status":"ok","timestamp":1530628775897,"user_tz":-540,"elapsed":1874,"user":{"displayName":"佐藤大輔","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102854220323104846215"}}},"cell_type":"code","source":["!pwd"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/practice/colaboratory/180519_snli_lstm_with_w2v_by_pytorch\r\n"],"name":"stdout"}]},{"metadata":{"id":"I4w-aNw7LF0G","colab_type":"text"},"cell_type":"markdown","source":["pytorch関連のインストール"]},{"metadata":{"id":"O9C17dxOZ0eb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":613},"outputId":"2906dcfb-0309-41e6-c12a-cbdf963d4173","executionInfo":{"status":"ok","timestamp":1530620679917,"user_tz":-540,"elapsed":48654,"user":{"displayName":"佐藤大輔","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102854220323104846215"}}},"cell_type":"code","source":["!pip3 install torch torchvision torchtext"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Collecting torch\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/43/380514bd9663f1bf708abeb359b8b48d3fabb1c8e95bb3427a980a064c57/torch-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (484.0MB)\n","\u001b[K    100% |████████████████████████████████| 484.0MB 27kB/s \n","tcmalloc: large alloc 1073750016 bytes == 0x5cc42000 @  0x7f0a5c7c51c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n","\u001b[?25hCollecting torchvision\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n","\u001b[K    100% |████████████████████████████████| 61kB 15.7MB/s \n","\u001b[?25hCollecting torchtext\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/90/474d5944d43001a6e72b9aaed5c3e4f77516fbef2317002da2096fd8b5ea/torchtext-0.2.3.tar.gz (42kB)\n","\u001b[K    100% |████████████████████████████████| 51kB 12.6MB/s \n","\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/24/f53ff6b61b3d728b90934bddb4f03f8ab584a7f49299bf3bde56e2952612/Pillow-5.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n","\u001b[K    100% |████████████████████████████████| 2.0MB 14.4MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.5)\n","Collecting tqdm (from torchtext)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/24/6ab1df969db228aed36a648a8959d1027099ce45fad67532b9673d533318/tqdm-4.23.4-py2.py3-none-any.whl (42kB)\n","\u001b[K    100% |████████████████████████████████| 51kB 12.4MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.18.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2018.4.16)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.6)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.22)\n","Building wheels for collected packages: torchtext\n","  Running setup.py bdist_wheel for torchtext ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/42/a6/f4/b267328bde6bb680094a0c173e8e5627ccc99543abded97204\n","Successfully built torchtext\n","Installing collected packages: torch, pillow, torchvision, tqdm, torchtext\n","  Found existing installation: Pillow 4.0.0\n","    Uninstalling Pillow-4.0.0:\n"],"name":"stdout"},{"output_type":"stream","text":["      Successfully uninstalled Pillow-4.0.0\n","Successfully installed pillow-5.2.0 torch-0.4.0 torchtext-0.2.3 torchvision-0.2.1 tqdm-4.23.4\n"],"name":"stdout"}]},{"metadata":{"id":"OCytU0JwcfWh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import os\n","import sys\n","import time\n","import glob\n","\n","import torch\n","import torch.optim as O\n","import torch.nn as nn\n","\n","from torchtext import data\n","from torchtext import datasets\n","\n","from model import SNLIClassifier\n","from util import get_args, makedirs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vrQvVOkYdSxh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":55},"outputId":"2af8269a-d594-4fa1-9588-dcdd75c4372b","executionInfo":{"status":"ok","timestamp":1530629880645,"user_tz":-540,"elapsed":703,"user":{"displayName":"佐藤大輔","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102854220323104846215"}}},"cell_type":"code","source":["sys.argv = [sys.argv[0],\n","            '--epochs', '10', '--d_embed', '300', # '--d_proj', '100', \n","            '--dp_ratio', '0.5', '--n_layers', '2',\n","            '--d_hidden', '300', # '--train_embed',\n","            '--no-bidirectional', '--no-projection',\n","            '--word_vectors', 'glove.6B.300d', \n","            '--vector_cache', '../input/vector_cache']\n","args = get_args()\n","args\n","# !python train.py --epochs 5 --d_embed 300 --gpu -1 --word_vectors glove.6B.300d"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Namespace(batch_size=128, birnn=False, d_embed=300, d_hidden=300, d_proj=300, dev_every=1000, dp_ratio=0.5, epochs=10, fix_emb=True, gpu=0, log_every=50, lower=True, lr=0.001, n_layers=2, projection=False, resume_snapshot='', save_every=1000, save_path='results', vector_cache='../input/vector_cache', word_vectors='glove.6B.300d')"]},"metadata":{"tags":[]},"execution_count":14}]},{"metadata":{"id":"aXEx2rEddZDZ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["torch.cuda.set_device(args.gpu)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DwXrUomkdhoc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["inputs = data.Field(lower=args.lower)\n","answers = data.Field(sequential=False, use_vocab=False)\n","train, dev, test = datasets.SNLI.splits(inputs, answers, root=\"../input\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tQidplGbeFsg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["inputs.build_vocab(train, dev, test)\n","\n","inputs.vocab.load_vectors(args.word_vectors, cache=args.vector_cache)\n","# if args.word_vectors:\n","#     if os.path.isfile(args.vector_cache):\n","#         inputs.vocab.vectors = torch.load(args.vector_cache)\n","#     else:\n","#         inputs.vocab.load_vectors(args.word_vectors)\n","#         makedirs(os.path.dirname(args.vector_cache))\n","#         torch.save(inputs.vocab.vectors, args.vector_cache)\n","\n","answers.build_vocab(train)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-ZdhphK-e2ih","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n","            (train, dev, test), batch_size=args.batch_size, device=args.gpu)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gj_1sDyzwxMw","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":53},"outputId":"939119d9-1f68-4f16-dcd4-5c4d3fbc6960","executionInfo":{"status":"ok","timestamp":1530631090945,"user_tz":-540,"elapsed":1367,"user":{"displayName":"佐藤大輔","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102854220323104846215"}}},"cell_type":"code","source":["answers.vocab.stoi"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["defaultdict(<function torchtext.vocab._default_unk_index>,\n","            {'<unk>': 0, 'contradiction': 2, 'entailment': 1, 'neutral': 3})"]},"metadata":{"tags":[]},"execution_count":26}]},{"metadata":{"id":"rLJMrtfWFl6U","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["config = args\n","config.n_embed = len(inputs.vocab)\n","config.d_out = len(answers.vocab)\n","config.n_cells = config.n_layers\n","\n","# double the number of cells for bidirectional networks\n","if config.birnn:\n","    config.n_cells *= 2\n","    \n","criterion = nn.CrossEntropyLoss()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2V7BBApB9v5t","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["if args.resume_snapshot:\n","    model = torch.load(args.resume_snapshot, map_location=lambda storage, locatoin: storage.cuda(args.gpu))\n","else:\n","    model = SNLIClassifier(config)\n","    if args.word_vectors:\n","        model.embed.weight.data.copy_(inputs.vocab.vectors)\n","        model.cuda(args.gpu)\n","\n","opt = O.Adam(model.parameters(), lr=args.lr)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Cqr9dq43FnIR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":4997},"outputId":"0e3f574d-9f17-4521-e5aa-9f08caf695ab","executionInfo":{"status":"error","timestamp":1530630948021,"user_tz":-540,"elapsed":992166,"user":{"displayName":"佐藤大輔","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102854220323104846215"}}},"cell_type":"code","source":["iterations = 0\n","start = time.time()\n","best_dev_acc = -1\n","train_iter.repeat = False\n","header = '  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy'\n","dev_log_template = ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{:8.6f},{:12.4f},{:12.4f}'.split(','))\n","log_template =     ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{},{:12.4f},{}'.split(','))\n","makedirs(args.save_path)\n","print(header)\n","\n","for epoch in range(args.epochs):\n","    train_iter.init_epoch()\n","    n_correct, n_total = 0, 0\n","    for batch_idx, batch in enumerate(train_iter):\n","\n","        # switch model to training mode, clear gradient accumulators\n","        model.train(); opt.zero_grad()\n","\n","        iterations += 1\n","\n","        # forward pass\n","        answer = model(batch)\n","\n","        # calculate accuracy of predictions in the current batch\n","        n_correct += (torch.max(answer, 1)[1].view(batch.label.size()) == batch.label).sum().item()\n","        n_total += batch.batch_size\n","        train_acc = 100. * n_correct/n_total\n","\n","        # calculate loss of the network output with respect to training labels\n","        loss = criterion(answer, batch.label)\n","\n","        # backpropagate and update optimizer learning rate\n","        loss.backward(); opt.step()\n","\n","        # checkpoint model periodically\n","        if iterations % args.save_every == 0:\n","            snapshot_prefix = os.path.join(args.save_path, 'snapshot')\n","            snapshot_path = snapshot_prefix + '_acc_{:.4f}_loss_{:.6f}_iter_{}_model.pt'.format(train_acc, loss.item(), iterations)\n","            torch.save(model, snapshot_path)\n","            for f in glob.glob(snapshot_prefix + '*'):\n","                if f != snapshot_path:\n","                    os.remove(f)\n","\n","        # evaluate performance on validation set periodically\n","        if iterations % args.dev_every == 0:\n","\n","            # switch model to evaluation mode\n","            model.eval(); dev_iter.init_epoch()\n","\n","            # calculate accuracy on validation set\n","            n_dev_correct, dev_loss = 0, 0\n","            with torch.no_grad():\n","                for dev_batch_idx, dev_batch in enumerate(dev_iter):\n","                     answer = model(dev_batch)\n","                     n_dev_correct += (torch.max(answer, 1)[1].view(dev_batch.label.size()) == dev_batch.label).sum().item()\n","                     dev_loss = criterion(answer, dev_batch.label)\n","            dev_acc = 100. * n_dev_correct / len(dev)\n","\n","            print(dev_log_template.format(time.time()-start,\n","                epoch, iterations, 1+batch_idx, len(train_iter),\n","                100. * (1+batch_idx) / len(train_iter), loss.item(), dev_loss.item(), train_acc, dev_acc))\n","\n","            # update best valiation set accuracy\n","            if dev_acc > best_dev_acc:\n","\n","                # found a model with better validation set accuracy\n","\n","                best_dev_acc = dev_acc\n","                snapshot_prefix = os.path.join(args.save_path, 'best_snapshot')\n","                snapshot_path = snapshot_prefix + '_devacc_{}_devloss_{}__iter_{}_model.pt'.format(dev_acc, dev_loss.item(), iterations)\n","\n","                # save model, delete previous 'best_snapshot' files\n","                torch.save(model, snapshot_path)\n","                for f in glob.glob(snapshot_prefix + '*'):\n","                    if f != snapshot_path:\n","                        os.remove(f)\n","\n","        elif iterations % args.log_every == 0:\n","\n","            # print progress message\n","            print(log_template.format(time.time()-start,\n","                epoch, iterations, 1+batch_idx, len(train_iter),\n","                100. * (1+batch_idx) / len(train_iter), loss.item(), ' '*8, n_correct/n_total*100, ' '*12))\n","\n","\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy\n","     5     0        50    50/4292        1% 1.205007               35.7031             \n","     9     0       100   100/4292        2% 1.050765               39.5469             \n","    12     0       150   150/4292        3% 1.085025               41.2552             \n","    15     0       200   200/4292        5% 1.008855               42.7422             \n","    18     0       250   250/4292        6% 0.952761               43.8188             \n","    21     0       300   300/4292        7% 1.026032               44.6172             \n","    24     0       350   350/4292        8% 1.010507               45.2522             \n","    27     0       400   400/4292        9% 1.018619               45.8535             \n","    30     0       450   450/4292       10% 0.923589               46.6319             \n","    33     0       500   500/4292       12% 1.008312               47.3031             \n","    37     0       550   550/4292       13% 0.920236               47.9077             \n","    40     0       600   600/4292       14% 0.947214               48.4583             \n","    43     0       650   650/4292       15% 0.895710               48.9892             \n","    46     0       700   700/4292       16% 0.907921               49.4676             \n","    49     0       750   750/4292       17% 0.951360               49.9281             \n","    52     0       800   800/4292       19% 0.860579               50.3516             \n","    55     0       850   850/4292       20% 0.820026               50.7656             \n","    58     0       900   900/4292       21% 0.961806               51.0321             \n","    61     0       950   950/4292       22% 0.891437               51.4062             \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  return Variable(arr, volatile=not train)\n"],"name":"stderr"},{"output_type":"stream","text":["    80     0      1000  1000/4292       23% 0.965674 1.095278      51.7508      44.8283\n","    96     0      1050  1050/4292       24% 0.900155               52.0365             \n","    99     0      1100  1100/4292       26% 0.874694               52.3047             \n","   102     0      1150  1150/4292       27% 0.941088               52.5571             \n","   105     0      1200  1200/4292       28% 0.918072               52.7949             \n","   108     0      1250  1250/4292       29% 0.890997               53.0181             \n","   111     0      1300  1300/4292       30% 0.833695               53.2224             \n","   114     0      1350  1350/4292       31% 0.907167               53.3924             \n","   117     0      1400  1400/4292       33% 0.860202               53.5681             \n","   120     0      1450  1450/4292       34% 0.851732               53.7527             \n","   123     0      1500  1500/4292       35% 0.868774               53.9271             \n","   126     0      1550  1550/4292       36% 0.947634               54.0983             \n","   129     0      1600  1600/4292       37% 0.852095               54.2500             \n","   132     0      1650  1650/4292       38% 0.935371               54.3968             \n","   135     0      1700  1700/4292       40% 1.032969               54.5666             \n","   139     0      1750  1750/4292       41% 0.840773               54.7263             \n","   142     0      1800  1800/4292       42% 0.916431               54.8837             \n","   145     0      1850  1850/4292       43% 0.830035               55.0063             \n","   148     0      1900  1900/4292       44% 0.850401               55.1225             \n","   151     0      1950  1950/4292       45% 0.859290               55.2620             \n","   169     0      2000  2000/4292       47% 0.785045 1.101295      55.4023      41.5363\n","   172     0      2050  2050/4292       48% 0.776168               55.5351             \n","   175     0      2100  2100/4292       49% 0.828259               55.6503             \n","   178     0      2150  2150/4292       50% 0.833354               55.7435             \n","   181     0      2200  2200/4292       51% 0.911961               55.8629             \n","   185     0      2250  2250/4292       52% 0.893434               55.9642             \n","   188     0      2300  2300/4292       54% 0.896789               56.0316             \n","   191     0      2350  2350/4292       55% 0.940128               56.1250             \n","   194     0      2400  2400/4292       56% 0.785343               56.2214             \n","   197     0      2450  2450/4292       57% 0.833062               56.3205             \n","   200     0      2500  2500/4292       58% 0.947816               56.4150             \n","   203     0      2550  2550/4292       59% 0.716258               56.5156             \n","   206     0      2600  2600/4292       61% 0.944719               56.5856             \n","   209     0      2650  2650/4292       62% 0.941664               56.6695             \n","   213     0      2700  2700/4292       63% 0.841574               56.7500             \n","   216     0      2750  2750/4292       64% 0.841776               56.8389             \n","   219     0      2800  2800/4292       65% 0.780733               56.9289             \n","   222     0      2850  2850/4292       66% 0.859938               57.0085             \n","   225     0      2900  2900/4292       68% 0.906837               57.1032             \n","   228     0      2950  2950/4292       69% 0.942165               57.1870             \n","   246     0      3000  3000/4292       70% 0.794153 1.081197      57.2896      51.8797\n","   261     0      3050  3050/4292       71% 0.860028               57.3822             \n","   264     0      3100  3100/4292       72% 0.763579               57.4632             \n","   267     0      3150  3150/4292       73% 0.867844               57.5312             \n","   270     0      3200  3200/4292       75% 0.782200               57.6018             \n","   273     0      3250  3250/4292       76% 0.827636               57.6685             \n","   276     0      3300  3300/4292       77% 0.877515               57.7536             \n"],"name":"stdout"},{"output_type":"stream","text":["   280     0      3350  3350/4292       78% 0.859386               57.8300             \n","   283     0      3400  3400/4292       79% 0.893050               57.9035             \n","   286     0      3450  3450/4292       80% 0.783868               57.9726             \n","   289     0      3500  3500/4292       82% 0.860650               58.0435             \n","   292     0      3550  3550/4292       83% 0.707422               58.1224             \n","   295     0      3600  3600/4292       84% 0.768325               58.2014             \n","   298     0      3650  3650/4292       85% 0.721168               58.2622             \n","   301     0      3700  3700/4292       86% 0.919090               58.3404             \n","   304     0      3750  3750/4292       87% 0.798066               58.4150             \n","   307     0      3800  3800/4292       89% 0.753990               58.4671             \n","   311     0      3850  3850/4292       90% 0.869735               58.5294             \n","   314     0      3900  3900/4292       91% 0.740709               58.5857             \n","   317     0      3950  3950/4292       92% 0.925149               58.6487             \n","   335     0      4000  4000/4292       93% 0.899320 1.080299      58.7146      54.7043\n","   350     0      4050  4050/4292       94% 0.859710               58.7772             \n","   353     0      4100  4100/4292       96% 0.923150               58.8449             \n","   356     0      4150  4150/4292       97% 0.780960               58.9053             \n","   359     0      4200  4200/4292       98% 0.786833               58.9546             \n","   362     0      4250  4250/4292       99% 0.870185               59.0107             \n","   367     1      4300     8/4292        0% 0.742719               66.2109             \n","   370     1      4350    58/4292        1% 0.785411               65.0997             \n","   373     1      4400   108/4292        3% 0.771922               65.2633             \n","   377     1      4450   158/4292        4% 0.793871               65.2393             \n","   380     1      4500   208/4292        5% 0.792341               65.0654             \n","   383     1      4550   258/4292        6% 0.811086               65.2102             \n","   386     1      4600   308/4292        7% 0.761453               65.2192             \n","   389     1      4650   358/4292        8% 0.779635               65.1362             \n","   392     1      4700   408/4292       10% 0.790922               65.1099             \n","   395     1      4750   458/4292       11% 0.718499               65.1747             \n","   398     1      4800   508/4292       12% 0.810044               65.0560             \n","   401     1      4850   558/4292       13% 0.731007               65.0230             \n","   405     1      4900   608/4292       14% 0.736269               64.9851             \n","   408     1      4950   658/4292       15% 0.691496               64.9316             \n","   426     1      5000   708/4292       16% 0.795918 1.095666      64.9199      33.8549\n","   429     1      5050   758/4292       18% 0.787092               64.9705             \n","   432     1      5100   808/4292       19% 0.956682               65.0004             \n","   435     1      5150   858/4292       20% 0.705574               65.0259             \n","   438     1      5200   908/4292       21% 0.836045               65.0210             \n","   441     1      5250   958/4292       22% 0.807664               65.0207             \n","   444     1      5300  1008/4292       23% 0.820372               64.9910             \n","   447     1      5350  1058/4292       25% 0.798484               65.0372             \n","   450     1      5400  1108/4292       26% 1.001078               65.0045             \n","   454     1      5450  1158/4292       27% 0.818235               65.0036             \n","   457     1      5500  1208/4292       28% 0.770380               65.0145             \n","   460     1      5550  1258/4292       29% 0.836488               64.9717             \n","   463     1      5600  1308/4292       30% 0.813262               64.9722             \n","   466     1      5650  1358/4292       32% 0.777518               64.9928             \n"],"name":"stdout"},{"output_type":"stream","text":["   469     1      5700  1408/4292       33% 0.871729               65.0146             \n","   472     1      5750  1458/4292       34% 0.734570               65.0206             \n","   475     1      5800  1508/4292       35% 0.843190               65.0686             \n","   478     1      5850  1558/4292       36% 0.919327               65.0579             \n","   481     1      5900  1608/4292       37% 0.804819               65.0653             \n","   485     1      5950  1658/4292       39% 0.808262               65.0761             \n","   503     1      6000  1708/4292       40% 0.754204 1.088112      65.0551      46.9722\n","   506     1      6050  1758/4292       41% 0.782723               65.0602             \n","   509     1      6100  1808/4292       42% 0.814925               65.0447             \n","   512     1      6150  1858/4292       43% 0.763954               65.0157             \n","   515     1      6200  1908/4292       44% 0.823815               65.0137             \n","   518     1      6250  1958/4292       46% 0.780559               65.0636             \n","   521     1      6300  2008/4292       47% 0.755073               65.0990             \n","   525     1      6350  2058/4292       48% 0.814670               65.1038             \n","   528     1      6400  2108/4292       49% 0.716815               65.1328             \n","   531     1      6450  2158/4292       50% 0.830826               65.1359             \n","   534     1      6500  2208/4292       51% 0.906816               65.1403             \n","   537     1      6550  2258/4292       53% 0.832528               65.1468             \n","   540     1      6600  2308/4292       54% 0.781087               65.1586             \n","   543     1      6650  2358/4292       55% 0.869847               65.1956             \n","   546     1      6700  2408/4292       56% 0.790164               65.2052             \n","   550     1      6750  2458/4292       57% 0.756599               65.2287             \n","   553     1      6800  2508/4292       58% 0.787365               65.2294             \n","   556     1      6850  2558/4292       60% 0.747158               65.2304             \n","   559     1      6900  2608/4292       61% 0.745773               65.2365             \n","   562     1      6950  2658/4292       62% 0.717465               65.2470             \n","   580     1      7000  2708/4292       63% 0.699010 1.097122      65.2456      34.0886\n","   583     1      7050  2758/4292       64% 0.740689               65.2746             \n","   586     1      7100  2808/4292       65% 0.777495               65.2942             \n","   589     1      7150  2858/4292       67% 0.722881               65.3158             \n","   592     1      7200  2908/4292       68% 0.708492               65.3397             \n","   595     1      7250  2958/4292       69% 0.731458               65.3511             \n","   598     1      7300  3008/4292       70% 0.667952               65.3655             \n","   601     1      7350  3058/4292       71% 0.681665               65.3828             \n","   604     1      7400  3108/4292       72% 0.769873               65.3922             \n","   608     1      7450  3158/4292       74% 0.735669               65.4058             \n","   611     1      7500  3208/4292       75% 0.737694               65.4143             \n","   614     1      7550  3258/4292       76% 0.708892               65.4425             \n","   617     1      7600  3308/4292       77% 0.748599               65.4415             \n","   620     1      7650  3358/4292       78% 0.742509               65.4635             \n","   623     1      7700  3408/4292       79% 0.838919               65.4847             \n","   626     1      7750  3458/4292       81% 0.718326               65.4870             \n","   629     1      7800  3508/4292       82% 0.795576               65.5016             \n","   632     1      7850  3558/4292       83% 0.718862               65.5104             \n","   635     1      7900  3608/4292       84% 0.795667               65.5193             \n","   638     1      7950  3658/4292       85% 0.803449               65.5310             \n","   656     1      8000  3708/4292       86% 0.705007 1.095820      65.5473      33.8244\n"],"name":"stdout"},{"output_type":"stream","text":["   659     1      8050  3758/4292       88% 0.794548               65.5551             \n","   662     1      8100  3808/4292       89% 0.803797               65.5710             \n","   665     1      8150  3858/4292       90% 0.814640               65.5861             \n","   668     1      8200  3908/4292       91% 0.695932               65.6000             \n","   672     1      8250  3958/4292       92% 0.896386               65.6157             \n","   675     1      8300  4008/4292       93% 0.698804               65.6250             \n","   678     1      8350  4058/4292       95% 0.792858               65.6310             \n","   681     1      8400  4108/4292       96% 0.727315               65.6364             \n","   684     1      8450  4158/4292       97% 0.717422               65.6485             \n","   687     1      8500  4208/4292       98% 0.705622               65.6753             \n","   690     1      8550  4258/4292       99% 0.794242               65.6861             \n","   695     2      8600    16/4292        0% 0.695194               68.1152             \n","   698     2      8650    66/4292        2% 0.715097               69.0814             \n","   702     2      8700   116/4292        3% 0.761815               68.6827             \n","   705     2      8750   166/4292        4% 0.708511               68.4300             \n","   708     2      8800   216/4292        5% 0.742465               68.4064             \n","   711     2      8850   266/4292        6% 0.651091               68.2331             \n","   714     2      8900   316/4292        7% 0.663052               68.2531             \n","   717     2      8950   366/4292        9% 0.909126               68.2164             \n","   735     2      9000   416/4292       10% 0.708179 1.109105      68.1378      33.8244\n","   738     2      9050   466/4292       11% 0.796331               68.0710             \n","   741     2      9100   516/4292       12% 0.778841               68.0687             \n","   744     2      9150   566/4292       13% 0.680088               68.0640             \n","   747     2      9200   616/4292       14% 0.834202               68.0182             \n","   750     2      9250   666/4292       16% 0.745373               68.0168             \n","   753     2      9300   716/4292       17% 0.864914               67.9884             \n","   757     2      9350   766/4292       18% 0.703186               67.9657             \n","   760     2      9400   816/4292       19% 0.699792               67.9697             \n","   763     2      9450   866/4292       20% 0.728010               67.9507             \n","   766     2      9500   916/4292       21% 0.694737               67.9926             \n","   769     2      9550   966/4292       23% 0.774752               67.9712             \n","   772     2      9600  1016/4292       24% 0.746147               67.9941             \n","   775     2      9650  1066/4292       25% 0.729383               68.0076             \n","   778     2      9700  1116/4292       26% 0.730461               67.9947             \n","   781     2      9750  1166/4292       27% 0.678604               68.0237             \n","   785     2      9800  1216/4292       28% 0.775073               68.0201             \n","   788     2      9850  1266/4292       29% 0.709127               68.0286             \n","   791     2      9900  1316/4292       31% 0.701679               68.0507             \n","   794     2      9950  1366/4292       32% 0.778059               68.0900             \n","   812     2     10000  1416/4292       33% 0.693694 1.145225      68.0929      33.8244\n","   815     2     10050  1466/4292       34% 0.736743               68.0721             \n","   818     2     10100  1516/4292       35% 0.758957               68.0703             \n","   821     2     10150  1566/4292       36% 0.658956               68.0585             \n","   824     2     10200  1616/4292       38% 0.731548               68.0577             \n","   827     2     10250  1666/4292       39% 0.671275               68.0883             \n","   830     2     10300  1716/4292       40% 0.798591               68.0867             \n","   834     2     10350  1766/4292       41% 0.763725               68.0979             \n"],"name":"stdout"},{"output_type":"stream","text":["   837     2     10400  1816/4292       42% 0.727212               68.0501             \n","   840     2     10450  1866/4292       43% 0.737566               68.0646             \n","   843     2     10500  1916/4292       45% 0.699483               68.0719             \n","   846     2     10550  1966/4292       46% 0.728310               68.0808             \n","   849     2     10600  2016/4292       47% 0.756892               68.0590             \n","   852     2     10650  2066/4292       48% 0.856493               68.0485             \n","   855     2     10700  2116/4292       49% 0.814746               68.0526             \n","   858     2     10750  2166/4292       50% 0.685603               68.0903             \n","   861     2     10800  2216/4292       52% 0.661667               68.0865             \n","   864     2     10850  2266/4292       53% 0.768069               68.0867             \n","   868     2     10900  2316/4292       54% 0.842363               68.0821             \n","   871     2     10950  2366/4292       55% 0.747094               68.0979             \n","   890     2     11000  2416/4292       56% 0.785692 1.131698      68.0852      33.8244\n","   893     2     11050  2466/4292       57% 0.584538               68.1132             \n","   896     2     11100  2516/4292       59% 0.644112               68.0892             \n","   899     2     11150  2566/4292       60% 0.752275               68.1012             \n","   902     2     11200  2616/4292       61% 0.725987               68.1145             \n","   905     2     11250  2666/4292       62% 0.901446               68.1188             \n","   908     2     11300  2716/4292       63% 0.697232               68.1195             \n","   912     2     11350  2766/4292       64% 0.712665               68.1182             \n","   915     2     11400  2816/4292       66% 0.747480               68.1155             \n","   918     2     11450  2866/4292       67% 0.823281               68.1097             \n","   921     2     11500  2916/4292       68% 0.683229               68.1156             \n","   924     2     11550  2966/4292       69% 0.788813               68.1215             \n","   927     2     11600  3016/4292       70% 0.725063               68.1174             \n","   930     2     11650  3066/4292       71% 0.660691               68.1109             \n","   933     2     11700  3116/4292       73% 0.745892               68.1114             \n","   936     2     11750  3166/4292       74% 0.689651               68.1343             \n","   939     2     11800  3216/4292       75% 0.755354               68.1347             \n","   942     2     11850  3266/4292       76% 0.773713               68.1199             \n","   946     2     11900  3316/4292       77% 0.805958               68.1273             \n","   949     2     11950  3366/4292       78% 0.680759               68.1400             \n","   966     2     12000  3416/4292       80% 0.657374 1.130364      68.1506      33.8244\n","   970     2     12050  3466/4292       81% 0.724788               68.1527             \n","   973     2     12100  3516/4292       82% 0.664128               68.1385             \n","   976     2     12150  3566/4292       83% 0.764335               68.1469             \n","   979     2     12200  3616/4292       84% 0.858031               68.1396             \n","   982     2     12250  3666/4292       85% 0.665386               68.1429             \n","   985     2     12300  3716/4292       87% 0.659921               68.1525             \n","   988     2     12350  3766/4292       88% 0.649249               68.1496             \n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-b477c58deb13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# backpropagate and update optimizer learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# checkpoint model periodically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"oqEp9p_wSo0-","colab_type":"text"},"cell_type":"markdown","source":["モデルを読み込んでテストデータで評価"]},{"metadata":{"id":"wUlEOfhzRPbB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":127},"outputId":"79c0323f-6d96-461f-8cec-927d67296544","executionInfo":{"status":"ok","timestamp":1530015521213,"user_tz":-540,"elapsed":2292,"user":{"displayName":"佐藤大輔","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102854220323104846215"}}},"cell_type":"code","source":["path_model_eval = \"results/best_snapshot_devacc_77.52489331436699_devloss_0.7100985050201416__iter_31000_model.pt\"\n","model_eval = torch.load(path_model_eval, \n","                        map_location=lambda storage, \n","                        locatoin: storage.cuda(args.gpu))\n","\n","def evaluate(model, data_iter, data_size):\n","    log_template = '{:>8.6f}, {:8.6f}, {:12.4f}%'\n","    model.eval(); data_iter.init_epoch()\n","  \n","    # calculate accuracy on validation set\n","    n_correct, loss = 0, 0\n","    with torch.no_grad():\n","        for batch_idx, batch in enumerate(data_iter):\n","            answer = model(batch)\n","            n_correct += (torch.max(answer, 1)[1].view(batch.label.size()) == batch.label).sum().item()\n","            loss = criterion(answer, batch.label)\n","    acc = 100. * n_correct / data_size\n","\n","    print(log_template.format(loss.item(), loss.item(), acc))\n","        \n","evaluate(model_eval, test_iter, len(test))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  return Variable(arr, volatile=not train)\n","/content/drive/practice/colaboratory/180519_snli_lstm_with_w2v_by_pytorch/model.py:34: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n","  outputs, (ht, ct) = self.rnn(inputs, (h0, c0))\n"],"name":"stderr"},{"output_type":"stream","text":["0.663549, 0.663549,      76.4047%\n"],"name":"stdout"}]},{"metadata":{"id":"k0ty6W3rJwp4","colab_type":"text"},"cell_type":"markdown","source":["google driveとの同期プロセスも完全にkillする"]},{"metadata":{"id":"rrGcu22-fX_h","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# !kill -9 -1"],"execution_count":0,"outputs":[]}]}